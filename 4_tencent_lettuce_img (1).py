# -*- coding: utf-8 -*-
"""4.Tencent_Lettuce_img.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13KpL7ubW5jyj4uA8EhaxcKtdGpio10QZ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import keras
from keras.layers import Dense, Conv2D, BatchNormalization, Activation
from keras.layers import AveragePooling2D, Input, Flatten
from keras.optimizers import Adam,Adagrad 
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model
from keras.callbacks import Callback

from keras import models
from keras.layers import core, convolutional, pooling

from keras.applications.resnet_v2 import ResNet50V2
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.models import Model,Input
from keras.layers import Dense, GlobalAveragePooling2D,Dropout,MaxPooling2D
from keras import backend as K,regularizers
from keras.optimizers import Adam
from keras.models import load_model

import os
import cv2
import json
import glob
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

###########################################################################3
#=============================DO NOT RE-RUN ,IT TAKES ABOUT 30 MINS ======================================================
y=pd.read_excel('D://dataset//tencent/aug_new.xlsx')
y['location']='D://dataset//tencent//aug//'+y['RGBImage']

filenames=y['location'].to_list()

scale_percent = 30 # percent of original size
width = int(1000 * scale_percent / 100)
height = int(830* scale_percent / 100)
dim = (width, height)

X=[]
for i in filenames:
    img =cv2.imread(i, 1)
    img=cv2.resize(img,dim, interpolation = cv2.INTER_AREA)
    img=np.array(img, dtype="float32")
    X.append(img)

X-= np.mean(X, axis = 0)
X=np.asarray(X)
np.save('D://dataset//tencent//fil.npy',X)
del X

X = np.load('/content/drive/My Drive/tencent/fil.npy')
Y=pd.read_excel('/content/drive/My Drive/tencent/Target.xlsx')
Y=Y.drop(['Variety','RGBImage','DebthInformation','location','lo'],axis=1)
print(Y)

#=======================separate train , val, test
def train_test_split(X,Y,num_training=1100,num_validation=200,num_test=246):
  Y1=Y.to_numpy()
  num_training=num_training
  num_validation=num_validation
  num_test=num_test
  #mask = list(range(num_training, num_training + num_validation))

  X_train = X[:num_training]
  Y_train=Y1[:num_training]

  X_val = X[num_training:num_training+num_validation]
  y_val=Y1[num_training:num_training+num_validation]

  X_test= X[num_training+num_validation:]
  Y_test=Y1[num_training+num_validation:]
  return X_train,Y_train,X_val,y_val,X_test,Y_test

X_train,Y_train,X_val,Y_val,X_test,Y_test=train_test_split(X,Y,num_training=1100,num_validation=200,num_test=246)
print(X_test.shape)

#=====save copy of label
Y1=pd.DataFrame(Y_train)
Y1.to_excel('/content/drive/My Drive/tencent/Y_train_real.xlsx')

Y2=pd.DataFrame(Y_test)
Y2.to_excel('/content/drive/My Drive/tencent/Y_test_real.xlsx')

X = np.load('/content/drive/My Drive/tencent/fil.npy')
Y=pd.read_excel('/content/drive/My Drive/tencent/Target.xlsx')
Y=Y.drop(['Variety','RGBImage','DebthInformation','location','lo'],axis=1)
print(Y)

#=======================Model building===============================baseline model===========================
base_model=InceptionV3(weights='imagenet',include_top=False)

# Create your own input format
x = base_model.output
x = GlobalAveragePooling2D()(x)
# add a fully-connected layer
x=Flatten()(x)
x=Dense(100, activation='relu')(x)
x=Dense(50, activation='relu')(x)
x=Dense(10, activation='relu')(x)
# and a logistic layer -- let's say we have 4 classes
predictions = Dense(5)(x)

model = Model(inputs=base_model.input, outputs=predictions)

#==================SEPERATE 5 DIFFERENT TRAINING TARGET
Y_train_dry_weigth=Y_train[:,0]
Y_train_fresh_weigth=Y_train[:,1]
Y_train_Height=Y_train[:,2]
Y_train_Diameter=Y_train[:,3]
Y_train_LeafArea=Y_train[:,4]

Y_val_dry_weigth=y_val[:,0]
Y_val_fresh_weigth=y_val[:,1]
Y_val_Height=y_val[:,2]
Y_val_Diameter=y_val[:,3]
Y_val_LeafArea=y_val[:,4]

Y_test_dry_weigth=Y_test[:,0]
Y_test_fresh_weigth=Y_test[:,1]
Y_testHeight=Y_test[:,2]
Y_test_Diameter=Y_test[:,3]
Y_test_LeafArea=Y_test[:,4]

print(Y_test_LeafArea.shape)
[Y_train_dry_weigth,Y_train_fresh_weigth,Y_train_Height,Y_train_Diameter,Y_train_LeafArea]
[Y_val_dry_weigth,Y_val_fresh_weigth,Y_val_Height,Y_val_Diameter,Y_val_LeafArea]

#=======================Model building
#base_model=ResNet50V2(weights='imagenet',include_top=False)
base_model=InceptionV3(weights='imagenet',include_top=False)
# Create your own input format
x = base_model.output
x = GlobalAveragePooling2D()(x)
x=Flatten()(x)
#============================Seperate head output for each regression problem

x1=Dense(500, activation='relu')(x)
x1=Dense(100, activation='relu')(x1)
x1=Dense(20, activation='relu')(x1)
predictions1 = Dense(1,name='Fresh_weigth')(x1)

x2=Dense(500, activation='relu')(x)
x2=Dense(100, activation='relu')(x2)
x2=Dense(20, activation='relu')(x2)
predictions2 = Dense(1,name='Dry_weigth')(x2)

x3=Dense(500, activation='relu')(x)
x3=Dense(100, activation='relu')(x3)
x3=Dense(20, activation='relu')(x3)
predictions3 = Dense(1,name='Height')(x3)

x4=Dense(500, activation='relu')(x)
x4=Dense(100, activation='relu')(x4)
x4=Dense(20, activation='relu')(x4)
predictions4 = Dense(1,name='Diameter')(x4)

x5=Dense(500, activation='relu')(x)
x5=Dense(100, activation='relu')(x5)
x5=Dense(20, activation='relu')(x5)
predictions5 = Dense(1,name='LeafArea')(x5)

model = Model(inputs=base_model.input, outputs=[predictions1,predictions2,predictions3,predictions4,predictions5])

def lr_schedule(epoch):
    """Learning Rate Schedule

    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.

    # Arguments
        epoch (int): The number of epochs

    # Returns
        lr (float32): learning rate
    """
    lr = 0.005
    if epoch > 10:
        lr *= 0.5e-3
    elif epoch > 20:
        lr *= 1e-3
    elif epoch > 30:
        lr *= 1e-2
    elif epoch > 40:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr

def custom_loss(y_true, y_pred):
    diff = K.sum(K.square(y_pred - y_true))
    bottom=K.sum(K.square(y_true))
    pert= diff/bottom
    return K.sum(pert)

filepath ='/content/drive/My Drive/tencent/weights_new2.hdf5' 

checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_loss',
                             verbose=1,
                             save_best_only=True)

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

callbacks = [checkpoint, lr_reducer, lr_scheduler]

def lr_schedule(epoch):
    """Learning Rate Schedule

    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.

    # Arguments
        epoch (int): The number of epochs

    # Returns
        lr (float32): learning rate
    """
    lr = 0.0005
    if epoch > 3:
        lr *= 1e-2
    elif epoch > 20:
        lr *= 1e-2
    elif epoch > 30:
        lr *= 1e-2
    elif epoch > 40:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr

def custom_loss(y_true, y_pred):
    diff = K.sum(K.square(y_pred - y_true))
    bottom=K.sum(K.square(y_true))
    pert= diff/bottom
    return K.sum(pert)

filepath ='/content/drive/My Drive/tencent/weights_new2.hdf5' 

checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_loss',
                             verbose=1,
                             save_best_only=True)

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

callbacks = [checkpoint, lr_reducer, lr_scheduler]

model.compile(optimizer='Adam', loss=custom_loss)
model.summary()

model.fit(X_train, 
          Y_train,
          epochs=200,
          validation_data=(X_val,Y_val),
          batch_size = 64,
          callbacks=callbacks)

model.fit(X_train, 
          [Y_train_dry_weigth,Y_train_fresh_weigth,Y_train_Height,Y_train_Diameter,Y_train_LeafArea],
          epochs=30,
          validation_data=(X_val,[Y_val_dry_weigth,Y_val_fresh_weigth,Y_val_Height,Y_val_Diameter,Y_val_LeafArea]),
          batch_size = 64,
          callbacks=callbacks)

model.save('/content/drive/My Drive/tencent/inceptionv3_single.h5')

model1=load_model('/content/drive/My Drive/tencent/inceptionv3_single.h5',
                 compile=False)
model1.compile(optimizer='adam', loss=)

Y_test=model.predict(X_test, batch_size=None, 
        verbose=0, 
        steps=None, 
        callbacks=None,
        max_queue_size=10, 
        workers=1, 
        use_multiprocessing=False)

y1=pd.DataFrame(Y_test)
print(y1)
y1.to_excel('/content/drive/My Drive/tencent/Y_test_output.xlsx')

model = models.Sequential()
model.add(convolutional.Convolution2D(16, 3, 3, input_shape=(249, 300, 3), activation='relu'))
model.add(pooling.MaxPooling2D(pool_size=(2, 2)))
model.add(convolutional.Convolution2D(32, 3, 3, activation='relu'))
model.add(pooling.MaxPooling2D(pool_size=(2, 2)))
model.add(convolutional.Convolution2D(64, 3, 3, activation='relu'))
model.add(pooling.MaxPooling2D(pool_size=(2, 2)))
model.add(core.Flatten())
model.add(core.Dense(500, activation='relu'))
model.add(core.Dense(100, activation='relu'))
model.add(core.Dense(20, activation='relu'))
model.add(core.Dense(5))

model.compile(optimizer='Adam', loss=custom_loss)
#model.summary()

model.fit(X_train, 
          Y_train,
          epochs=60,
          validation_data=(X_val,y_val),
          batch_size = 64,
          callbacks=callbacks)