# -*- coding: utf-8 -*-
"""1.Tencent_Lettuce_distribution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kG9rcSEI5SSfEDNeAOEnLzJBjaY1dUq-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
import random, re, math
import tensorflow as tf, tensorflow.keras.backend as K
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers
import cv2
import json
import numpy as np
import pandas as pd
import os
import random, re, math

import json
import glob
import cv2
import scipy
from sklearn.preprocessing import StandardScaler
import scipy.stats
import matplotlib.pyplot as plt
# %matplotlib inline

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/tencent/'
path_img='/content/drive/My Drive/tencent/img/*.jpg'
path='/content/drive/My Drive/tencent/GroundTruth.json'

def pre_process_y(path):
    with open(path,'r') as f:
        data = json.loads(f.read())
    
    Image_list = list(data["Measurements"].keys())
    Field_list=list(data["Measurements"]["Image27"].keys())
    
    df2=[]
    for item in Field_list:
        df=[]    
        for img in Image_list:
            a=data["Measurements"][img][item]
            df.append(a)
        array=np.array(df)
        df2.append(array)
    
    df3=np.array(df2)    
    df=np.transpose(df3)
    data_out=pd.DataFrame(df,columns=['Variety', 'RGBImage', 'DebthInformation','FreshWeightShoot','DryWeightShoot',
                                'Height','Diameter','LeafArea'])
    return data_out

data_set =pre_process_y(path)
print(data_set)
data_set.columns
y_all=pd.DataFrame(data_set[['FreshWeightShoot','DryWeightShoot','Height','Diameter','LeafArea']])
print(y_LeafArea)

#=====describe distribution
plt.hist(y_LeafArea)
plt.show()
cols = y_all.columns
y_all[cols] = y_all[cols].apply(pd.to_numeric, errors='coerce')
b=y_all.describe()
b.to_excel('test.xlsx')

#====standardise
y_fresh_weight=np.array(y_all['Height'])
yy = y_fresh_weight.reshape (-1,1)

sc=StandardScaler() 
sc.fit(yy)
y_std =sc.transform(yy)
y_std = y_std.flatten()
#y_std = y_std.flatten()

#y_std=y_std.sort()
y_std
# Set up 50 bins for chi-square test
# Observed data will be approximately evenly distrubuted aross all bins
percentile_bins = np.linspace(0,100,51)
percentile_cutoffs = np.percentile(y_std, percentile_bins)
observed_frequency, bins = (np.histogram(y_std, bins=percentile_cutoffs))

# Turn off code warnings (this is not recommended for routine use)
import warnings
warnings.filterwarnings("ignore")


dist_names = ['beta',
              'expon',
              'gamma',
              'lognorm',
              'norm',
              'pearson3',
              'triang',
              'uniform',
              'weibull_min', 
              'weibull_max']

# Set up empty lists to stroe results
chi_square = []
p_values = []
size = len(y_std)

# Set up 50 bins for chi-square test
# Observed data will be approximately evenly distrubuted aross all bins
percentile_bins = np.linspace(0,100,51)
percentile_cutoffs = np.percentile(y_std, percentile_bins)
observed_frequency, bins = (np.histogram(y_std, bins=percentile_cutoffs))
cum_observed_frequency = np.cumsum(observed_frequency)

for distribution in dist_names:
    # Set up distribution and get fitted distribution parameters
    dist = getattr(scipy.stats, distribution)
    param = dist.fit(y_std)
    
    # Obtain the KS test P statistic, round it to 5 decimal places
    p = scipy.stats.kstest(y_std, distribution, args=param)[1]
    p = np.around(p, 5)
    p_values.append(p)    
    
    # Get expected counts in percentile bins
    # This is based on a 'cumulative distrubution function' (cdf)
    cdf_fitted = dist.cdf(percentile_cutoffs, *param[:-2], loc=param[-2], 
                          scale=param[-1])
    expected_frequency = []
    for bin in range(len(percentile_bins)-1):
        expected_cdf_area = cdf_fitted[bin+1] - cdf_fitted[bin]
        expected_frequency.append(expected_cdf_area)
    
    # calculate chi-squared
    expected_frequency = np.array(expected_frequency) * size
    cum_expected_frequency = np.cumsum(expected_frequency)
    ss = sum (((cum_expected_frequency - cum_observed_frequency) ** 2) / cum_observed_frequency)
    chi_square.append(ss)
        
# Collate results and sort by goodness of fit (best at top)

results = pd.DataFrame()
results['Distribution'] = dist_names
results['chi_square'] = chi_square
results['p_value'] = p_values
results.sort_values(['chi_square'], inplace=True)
    
# Report results
results.to_excel('test.xlsx')
print ('\nDistributions sorted by goodness of fit:')
print ('----------------------------------------')
print (results)

# Divide the observed data into 100 bins for plotting (this can be changed)
y=y_fresh_weight
x = np.arange(len(y))
number_of_bins = 100
bin_cutoffs = np.linspace(np.percentile(y,0), np.percentile(y,99),number_of_bins)

# Create the plot
h = plt.hist(y, bins = bin_cutoffs, color='0.75')

# Get the top three distributions from the previous phase
number_distributions_to_plot = 3
dist_names = results['Distribution'].iloc[0:number_distributions_to_plot]

# Create an empty list to stroe fitted distribution parameters
parameters = []

# Loop through the distributions ot get line fit and paraemters

for dist_name in dist_names:
    # Set up distribution and store distribution paraemters
    dist = getattr(scipy.stats, dist_name)
    param = dist.fit(y)
    parameters.append(param)
    
    # Get line for each distribution (and scale to match observed data)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1])
    scale_pdf = np.trapz (h[0], h[1][:-1]) / np.trapz (pdf_fitted, x)
    pdf_fitted *= scale_pdf
    
    # Add the line to the plot
    plt.plot(pdf_fitted, label=dist_name)
    
    # Set the plot x axis to contain 99% of the data
    # This can be removed, but sometimes outlier data makes the plot less clear
    plt.xlim(0,np.percentile(y,99))

# Add legend and display plot

plt.legend()
plt.show()

# Store distribution paraemters in a dataframe (this could also be saved)
dist_parameters = pd.DataFrame()
dist_parameters['Distribution'] = (
        results['Distribution'].iloc[0:number_distributions_to_plot])
dist_parameters['Distribution parameters'] = parameters

# Print parameter results
print ('\nDistribution parameters:')
print ('------------------------')

for index, row in dist_parameters.iterrows():
    print ('\nDistribution:', row[0])
    print ('Parameters:', row[1] )