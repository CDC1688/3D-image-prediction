# -*- coding: utf-8 -*-
"""1.Tencent_Lettuce_distribution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13iTiNh43F0Z1Ei5nHlV5Jcl-vv7ZCiPV
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
import random, re, math
import tensorflow as tf, tensorflow.keras.backend as K
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers
import cv2
import json
import numpy as np
import pandas as pd
import os
import random, re, math

import json
import glob
import cv2
import scipy
from sklearn.preprocessing import StandardScaler
import scipy.stats
import matplotlib.pyplot as plt
# %matplotlib inline
!pip install pip install open3d
from open3d import *
import open3d as o3d

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

from open3d import *
import open3d as o3d
#color_raw = o3d.io.read_image("/content/drive/My Drive/tencent/d3/RGB_22.png")
#depth_raw = o3d.io.read_image("/content/drive/My Drive/tencent/d3/Debth_22.png")
color_raw = cv2.imread("/content/drive/My Drive/tencent/d3/RGB_22.png")
depth_raw = cv2.imread("/content/drive/My Drive/tencent/d3/Debth_22.png")

o3d_image = o3d.geometry.Image(color_raw.astype(np.float32))
o3d_depth = o3d.geometry.Image(depth_raw.astype(np.float32))

rgbd_image = open3d.geometry.RGBDImage.create_from_color_and_depth(o3d_image, o3d_depth)

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/tencent/'
path_img='/content/drive/My Drive/tencent/img/*.jpg'
path='/content/drive/My Drive/tencent/GroundTruth.json'

def pre_process_y(path):
    with open(path,'r') as f:
        data = json.loads(f.read())
    
    Image_list = list(data["Measurements"].keys())
    Field_list=list(data["Measurements"]["Image27"].keys())
    
    df2=[]
    for item in Field_list:
        df=[]    
        for img in Image_list:
            a=data["Measurements"][img][item]
            df.append(a)
        array=np.array(df)
        df2.append(array)
    
    df3=np.array(df2)    
    df=np.transpose(df3)
    data_out=pd.DataFrame(df,columns=['Variety', 'RGBImage', 'DebthInformation','FreshWeightShoot','DryWeightShoot',
                                'Height','Diameter','LeafArea'])
    return data_out

data_set =pre_process_y(path)
print(data_set)
data_set.columns
y_all=pd.DataFrame(data_set[['FreshWeightShoot','DryWeightShoot','Height','Diameter','LeafArea']])
print(y_LeafArea)

#=====describe distribution
plt.hist(y_LeafArea)
plt.show()
cols = y_all.columns
y_all[cols] = y_all[cols].apply(pd.to_numeric, errors='coerce')
b=y_all.describe()
b.to_excel('test.xlsx')

#====standardise
y_fresh_weight=np.array(y_all['Height'])
yy = y_fresh_weight.reshape (-1,1)

sc=StandardScaler() 
sc.fit(yy)
y_std =sc.transform(yy)
y_std = y_std.flatten()
#y_std = y_std.flatten()

#y_std=y_std.sort()
y_std
# Set up 50 bins for chi-square test
# Observed data will be approximately evenly distrubuted aross all bins
percentile_bins = np.linspace(0,100,51)
percentile_cutoffs = np.percentile(y_std, percentile_bins)
observed_frequency, bins = (np.histogram(y_std, bins=percentile_cutoffs))

# Turn off code warnings (this is not recommended for routine use)
import warnings
warnings.filterwarnings("ignore")


dist_names = ['beta',
              'expon',
              'gamma',
              'lognorm',
              'norm',
              'pearson3',
              'triang',
              'uniform',
              'weibull_min', 
              'weibull_max']

# Set up empty lists to stroe results
chi_square = []
p_values = []
size = len(y_std)

# Set up 50 bins for chi-square test
# Observed data will be approximately evenly distrubuted aross all bins
percentile_bins = np.linspace(0,100,51)
percentile_cutoffs = np.percentile(y_std, percentile_bins)
observed_frequency, bins = (np.histogram(y_std, bins=percentile_cutoffs))
cum_observed_frequency = np.cumsum(observed_frequency)

for distribution in dist_names:
    # Set up distribution and get fitted distribution parameters
    dist = getattr(scipy.stats, distribution)
    param = dist.fit(y_std)
    
    # Obtain the KS test P statistic, round it to 5 decimal places
    p = scipy.stats.kstest(y_std, distribution, args=param)[1]
    p = np.around(p, 5)
    p_values.append(p)    
    
    # Get expected counts in percentile bins
    # This is based on a 'cumulative distrubution function' (cdf)
    cdf_fitted = dist.cdf(percentile_cutoffs, *param[:-2], loc=param[-2], 
                          scale=param[-1])
    expected_frequency = []
    for bin in range(len(percentile_bins)-1):
        expected_cdf_area = cdf_fitted[bin+1] - cdf_fitted[bin]
        expected_frequency.append(expected_cdf_area)
    
    # calculate chi-squared
    expected_frequency = np.array(expected_frequency) * size
    cum_expected_frequency = np.cumsum(expected_frequency)
    ss = sum (((cum_expected_frequency - cum_observed_frequency) ** 2) / cum_observed_frequency)
    chi_square.append(ss)
        
# Collate results and sort by goodness of fit (best at top)

results = pd.DataFrame()
results['Distribution'] = dist_names
results['chi_square'] = chi_square
results['p_value'] = p_values
results.sort_values(['chi_square'], inplace=True)
    
# Report results
results.to_excel('test.xlsx')
print ('\nDistributions sorted by goodness of fit:')
print ('----------------------------------------')
print (results)

# Divide the observed data into 100 bins for plotting (this can be changed)
y=y_fresh_weight
x = np.arange(len(y))
number_of_bins = 100
bin_cutoffs = np.linspace(np.percentile(y,0), np.percentile(y,99),number_of_bins)

# Create the plot
h = plt.hist(y, bins = bin_cutoffs, color='0.75')

# Get the top three distributions from the previous phase
number_distributions_to_plot = 3
dist_names = results['Distribution'].iloc[0:number_distributions_to_plot]

# Create an empty list to stroe fitted distribution parameters
parameters = []

# Loop through the distributions ot get line fit and paraemters

for dist_name in dist_names:
    # Set up distribution and store distribution paraemters
    dist = getattr(scipy.stats, dist_name)
    param = dist.fit(y)
    parameters.append(param)
    
    # Get line for each distribution (and scale to match observed data)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1])
    scale_pdf = np.trapz (h[0], h[1][:-1]) / np.trapz (pdf_fitted, x)
    pdf_fitted *= scale_pdf
    
    # Add the line to the plot
    plt.plot(pdf_fitted, label=dist_name)
    
    # Set the plot x axis to contain 99% of the data
    # This can be removed, but sometimes outlier data makes the plot less clear
    plt.xlim(0,np.percentile(y,99))

# Add legend and display plot

plt.legend()
plt.show()

# Store distribution paraemters in a dataframe (this could also be saved)
dist_parameters = pd.DataFrame()
dist_parameters['Distribution'] = (
        results['Distribution'].iloc[0:number_distributions_to_plot])
dist_parameters['Distribution parameters'] = parameters

# Print parameter results
print ('\nDistribution parameters:')
print ('------------------------')

for index, row in dist_parameters.iterrows():
    print ('\nDistribution:', row[0])
    print ('Parameters:', row[1] )

nb_classes = 4
BATCH_SIZE = 8 * strategy.num_replicas_in_sync
img_size = 768
EPOCHS = 40

LR_START = 0.00001
LR_MAX = 0.0001 * strategy.num_replicas_in_sync
LR_MIN = 0.00001
LR_RAMPUP_EPOCHS = 15
LR_SUSTAIN_EPOCHS = 3
LR_EXP_DECAY = .8

def lrfn(epoch):
    if epoch < LR_RAMPUP_EPOCHS:
        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START
    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:
        lr = LR_MAX
    else:
        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN
    return lr

    
lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)

def get_model():
    base_model =  efn.EfficientNetB7(weights='imagenet', include_top=False, pooling='avg', input_shape=(img_size, img_size, 3))
    x = base_model.output
    predictions = Dense(nb_classes, activation="softmax")(x)
    return Model(inputs=base_model.input, outputs=predictions)

with strategy.scope():
    model = get_model()
    
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])

# Data access
GCS_DS_PATH = 'gs://data_plant1'
train = pd.read_csv('/content/drive/My Drive/plant/train.csv')

train_paths = train.image_id.apply(lambda x: GCS_DS_PATH + '/train/' +x + '.jpg').values
train_labels = train.loc[:, 'healthy':].values
nb_classes = 4
BATCH_SIZE = 8 * strategy.num_replicas_in_sync
img_size = 768
EPOCHS = 40
print(train_paths)

def decode_image(filename, label=None, image_size=(img_size, img_size)):
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits, channels=3)
    image = tf.cast(image, tf.float32) / 255
    image = tf.image.resize(image, image_size)
    if label is None:
        return image
    else:
        return image, label
    
def data_augment(image, label=None, seed=2020):
    image = tf.image.random_flip_left_right(image, seed=seed)
    image = tf.image.random_flip_up_down(image, seed=seed)
           
    if label is None:
        return image
    else:
        return image, label

train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((train_paths, train_labels))
    .map(decode_image, num_parallel_calls=AUTO)
    .map(data_augment, num_parallel_calls=AUTO)
    .repeat()
    .shuffle(512)
    .batch(BATCH_SIZE)
    .prefetch(AUTO)
    )

model.fit(
    train_dataset,
    epochs=40,
    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,
    callbacks=[lr_callback]
)

base_model=InceptionResNetV2(weights='imagenet',include_top=False)

# add a global spatial average pooling layer
x = base_model.output
x = GlobalAveragePooling2D()(x)
# let's add a fully-connected layer
x = Dense(1024, activation='relu',kernel_initializer='he_uniform')(x)
x = Dropout(0.2)(x)
# and a logistic layer -- let's say we have 4 classes
predictions = Dense(4, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False
    
model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])
# train the model on the new data for a few epochs
model.fit(Xtrain, Ytrain, 
                    epochs=4,batch_size = 64)
# run for loss reach about 1 k

for i, layer in enumerate(model.layers):
   print(i, layer.name)

for layer in model.layers[:50]:
   layer.trainable = False
for layer in model.layers[50:]:
   layer.trainable = True

model.compile(optimizer=Adam(lr=lr_schedule(0)), 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(Xtrain, Ytrain, 
                    epochs=400,validation_data=(Xval,Yval),batch_size = 14,
          callbacks=callbacks)

from keras.models import load_model

model=load_model('/content/drive/My Drive/plant/weights2.hdf5')

def lr_schedule(epoch):
    """Learning Rate Schedule

    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.

    # Arguments
        epoch (int): The number of epochs

    # Returns
        lr (float32): learning rate
    """
    lr = 0.00025
    if epoch > 1000:
        lr = 0.000005
    elif epoch > 700:
        lr = 0.000005
    elif epoch > 200:
        lr = 0.00005
    elif epoch > 100:
        lr = 0.0001
    print('Learning rate: ', lr)
    return lr

filepath ="/content/drive/My Drive/plant/weights2.hdf5"

checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=True)

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

callbacks = [checkpoint,]

beta=0.9
number=1-np.power(beta,91)
weight=(1-beta)/number
w1=weight/number

number2=1-np.power(beta,622)
weight2=(1-beta)/number2
w2=weight2/number2

number3=1-np.power(beta,516)
weight3=(1-beta)/number3
w3=weight3/number3

number4=1-np.power(beta,592)
weight4=(1-beta)/number4
w4=weight4/number4

weighttotal=(w1+w2+w3+w4)
w1=w1/weighttotal
w2=w2/weighttotal
w3=w3/weighttotal
w4=w4/weighttotal


class_weight = {0: w1,
                1: w2,
                2: w3,
                3: w4}

