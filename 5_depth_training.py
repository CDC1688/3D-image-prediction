# -*- coding: utf-8 -*-
"""5.Depth_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gn3LR2XfCmEs2peBnJGQhOfPVotYm88e
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import keras
from keras.layers import Dense, Conv2D, BatchNormalization, Activation
from keras.layers import AveragePooling2D, Input, Flatten
from keras.optimizers import Adam,Adagrad 
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model
from keras.callbacks import Callback
from keras.applications.vgg16 import VGG16
from keras import models
from keras.layers import core, convolutional, pooling,concatenate

from keras.applications.resnet_v2 import ResNet50V2
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.models import Model,Input
from keras.layers import Dense, GlobalAveragePooling2D,Dropout,MaxPooling2D
from keras import backend as K,regularizers
from keras.optimizers import Adam
from keras.models import load_model

import os
import cv2
import json
import glob
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

X_depth=np.load('/content/drive/My Drive/tencent/fil_depth.npy')
print(X_depth.shape)
Y=pd.read_excel('/content/drive/My Drive/tencent/aug_new_shuffle.xlsx')
print(Y.columns)

Y=Y.drop(['Unnamed: 0','Unnamed: 0.1','Variety','RGBImage','DebthInformation','RGBImage'],axis=1)
print(Y.shape)
print(Y)

X = np.load('/content/drive/My Drive/tencent/fil.npy')
print(X.shape)


def train_test_split(X,Y,num_training=2500,num_validation=226):
  Y1=Y.to_numpy()
  num_training=num_training
  num_validation=num_validation
  #num_test=num_test
  #mask = list(range(num_training, num_training + num_validation))

  X_train = X[:num_training]
  Y_train=Y1[:num_training]

  X_val = X[num_training:num_training+num_validation]
  Y_val=Y1[num_training:num_training+num_validation]

  #X_test= X[num_training+num_validation:]
  #Y_test=Y1[num_training+num_validation:]
  return X_train,Y_train,X_val,Y_val

X_train,Y_train,X_val,Y_val=train_test_split(X,Y,num_training=2500,num_validation=226)
X_train_d,Y_train_d,X_val_d,Y_val_d=train_test_split(X_depth,Y,num_training=2500,num_validation=226)

#=======================Model building
base_model=InceptionV3(weights='imagenet',include_top=False)
# Create your own input format
x = base_model.output
x = GlobalAveragePooling2D()(x)
x=Flatten()(x)

img_input = Input(shape=(249, 300, 1))
img_conc = concatenate([img_input, img_input, img_input])  

base_model2=ResNet50V2(weights='imagenet',input_tensor=img_conc,include_top=False)

x1 = base_model2.output
x1 = GlobalAveragePooling2D()(x1)
x1=Flatten()(x1)

concat = concatenate([x,x1])
dense1 = Dense(500, activation = 'relu')(concat)
dense2 = Dense(100, activation = 'relu')(dense1)
dense3 = Dense(20, activation = 'relu')(dense2)
output = Dense(5,name='Output')(dense3)

model = Model(inputs=[base_model.input,base_model2.input], outputs=output)


def custom_loss(y_true, y_pred):
    diff = K.sum(K.square(y_pred - y_true),axis=0)
    bottom=K.sum(K.square(y_true),axis=0)
    pert= diff/bottom
    return K.sum(pert)

model.compile(optimizer='adam',loss=custom_loss)
model.summary()

def lr_schedule(epoch):
    """Learning Rate Schedule

    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.

    # Arguments
        epoch (int): The number of epochs

    # Returns
        lr (float32): learning rate
    """
    lr = 0.0005
    if epoch > 20:
       lr *= 0.5
    elif epoch > 50:
        lr *= 0.5
    elif epoch > 70:
        lr *= 0.5
    elif epoch > 100:
        lr *= 0.5
    elif epoch > 200:
        lr *= 0.5
    elif epoch > 250:
        lr *= 0.5
    elif epoch > 300:
        lr *= 0.5
    elif epoch > 350:
        lr *= 0.5
    elif epoch > 500:
        lr *= 0.5
    print('Learning rate: ', lr)
    return lr

filepath ='/content/drive/My Drive/tencent/weights_new.h5' 

checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_loss',
                             verbose=1,
                             save_best_only=True)

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

callbacks = [checkpoint, lr_reducer, lr_scheduler]

print(X_train_d.shape)
print(X_train.shape)
model.fit([X_train,X_train_d],
          Y_train,
          epochs=400,
          validation_data=([X_val,X_val_d],Y_val),
          batch_size = 64,
          callbacks=callbacks)
model.save('/content/drive/My Drive/tencent/Branch_new_all.h5')

model=load_model('/content/drive/My Drive/tencent/Branch2seperate_new.h5',
                 compile=False)


def custom_loss(y_true, y_pred):
    diff = K.sum(K.square(y_pred - y_true),axis=0)
    bottom=K.sum(K.square(y_true),axis=0)
    pert= diff/bottom
    return K.sum(pert)


model.compile(optimizer='adam', loss=custom_loss)     
#=====================read in json file
path_json='/content/drive/My Drive/tencent/test1/Images.json'

#===================Read in file order
def pre_process_y(path):
    with open(path,'r') as f:
        data = json.loads(f.read())
    
    Image_list = list(data["Measurements"].keys())
    Field_list=list(data["Measurements"]["Image1"].keys())
    
    df2=[]
    for item in Field_list:
        df=[]    
        for img in Image_list:
            a=data["Measurements"][img][item]
            df.append(a)
        array=np.array(df)
        df2.append(array)
    
    df3=np.array(df2)    
    df=np.transpose(df3)
    data_out=pd.DataFrame(df,columns=['RGBImage', 'DebthInformation'])
    return data_out,data

Y_out,data=pre_process_y(path_json)
Y_out['file']='/content/drive/My Drive/tencent/test1/'+Y_out['DebthInformation']
Y_out['location']='/content/drive/My Drive/tencent/test1/'+Y_out['RGBImage']

filenames_depth=Y_out['file'].to_list()
filenames_img=Y_out['location'].to_list()

#==================define scale
scale_percent = 30 # percent of original size
width = int(1000 * scale_percent / 100)
height = int(830* scale_percent / 100)
dim = (width, height)


#===================image array
X=[]
for i in filenames_img:
    img =cv2.imread(i, 1)
    img_crop=img[150:980,600:1600]   #crop unecessary 
    img=cv2.resize(img_crop,dim, interpolation = cv2.INTER_AREA)
    img=np.array(img, dtype="float64")
    X.append(img)
    
mean_image = np.mean(X, axis=0)
X-= mean_image
X=np.asarray(X)
#np.save('/content/drive/My Drive/tencent/final_test.npy',X)

#=====================Depth array
Depth=[]
for i in filenames_depth:
    depth = cv2.imread(i, -1)
    #depth1=depth.astype(float)
    depth_crop=depth[150:980,600:1600]
    depth_small=cv2.resize(depth_crop,dim,interpolation = cv2.INTER_AREA)
    depth_small=np.array(depth_small, dtype="float64")
    Depth.append(depth_small)

Depth=np.asarray(Depth)
Depth1=Depth[:,:,:,np.newaxis]
print(X.shape)
print(Depth1.shape)

#====================predict================================================
Y_test=model.predict([X,Depth1], batch_size=None, 
        verbose=0, 
        steps=None, 
        callbacks=None,
        max_queue_size=10, 
        workers=1, 
        use_multiprocessing=False)
print(Y_test)
Y=pd.DataFrame(Y_test)
Y.to_excel('/content/drive/My Drive/tencent/result_new.xlsx')
#========================result to JSON============================================
Image_list= list(data["Measurements"].keys())
    
for i in range(len(Y)):
    image=Image_list[i]
    data["Measurements"][image]['FreshWeightShoot']=Y[0][i]
    data["Measurements"][image]['DryWeightShoot']=Y[1][i]
    data["Measurements"][image]['Height']=Y[2][i]
    data["Measurements"][image]['Diameter']=Y[3][i]
    data["Measurements"][image]['LeafArea']=Y[4][i]

#=====get rid of General
data.pop('General')

file_output='/content/drive/My Drive/tencent/result_new.json'

with open(file_output, 'w') as json_file:
    json.dump(data,json_file,sort_keys=True,indent=4,separators=(',', ': '))

Y_train_pred=model.predict([X,X_depth], batch_size=None, 
        verbose=0, 
        steps=None, 
        callbacks=None,
        max_queue_size=10, 
        workers=1, 
        use_multiprocessing=False)
print(Y_train_pred)
Y_train=pd.DataFrame(Y_train_pred)
Y_train.to_excel('/content/drive/My Drive/tencent/result_train.xlsx')