# -*- coding: utf-8 -*-
"""Test_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vVHWu35gV5D5VKJJpqfOEfvvKEHO6L4U
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import keras
from keras.layers import Dense, Conv2D, BatchNormalization, Activation
from keras.layers import AveragePooling2D, Input, Flatten
from keras.optimizers import Adam,Adagrad 
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model
from keras.callbacks import Callback
from keras.applications.vgg16 import VGG16
from keras import models
from keras.layers import core, convolutional, pooling,concatenate

from keras.applications.resnet_v2 import ResNet50V2
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.models import Model,Input
from keras.layers import Dense, GlobalAveragePooling2D,Dropout,MaxPooling2D
from keras import backend as K,regularizers
from keras.optimizers import Adam
from keras.models import load_model

import os
import cv2
import json
import glob
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

#=======================Model building
base_model=InceptionV3(weights='imagenet',include_top=False)
# Create your own input format
x = base_model.output
x = GlobalAveragePooling2D()(x)
x=Flatten()(x)

img_input = Input(shape=(249, 300, 1))
img_conc = concatenate([img_input, img_input, img_input])  

base_model2=ResNet50V2(weights='imagenet',input_tensor=img_conc,include_top=False)

x1 = base_model2.output
x1 = GlobalAveragePooling2D()(x1)
x1=Flatten()(x1)

concat = concatenate([x,x1])
dense1 = Dense(500, activation = 'relu')(concat)
dense2 = Dense(100, activation = 'relu')(dense1)
dense3 = Dense(20, activation = 'relu')(dense2)
output = Dense(5,name='Output')(dense3)

model = Model(inputs=[base_model.input,base_model2.input], outputs=output)

def custom_loss(y_true, y_pred):
    diff = K.sum(K.square(y_pred - y_true),axis=0)
    bottom=K.sum(K.square(y_true),axis=0)
    pert= diff/bottom
    return K.sum(pert)

model.compile(optimizer='adam',loss=custom_loss)

model.load_weights('/content/drive/My Drive/tencent/weights_new.h5')

path_json='/content/drive/My Drive/tencent/test1/Images.json' 

#===================Read in file order
def pre_process_y(path):
    with open(path,'r') as f:
        data = json.loads(f.read())
    
    Image_list = list(data["Measurements"].keys())
    Field_list=list(data["Measurements"]["Image1"].keys())
    
    df2=[]
    for item in Field_list:
        df=[]    
        for img in Image_list:
            a=data["Measurements"][img][item]
            df.append(a)
        array=np.array(df)
        df2.append(array)
    
    df3=np.array(df2)    
    df=np.transpose(df3)
    data_out=pd.DataFrame(df,columns=['RGBImage', 'DebthInformation'])
    return data_out,data

Y_out,data=pre_process_y(path_json)
Y_out['file']='/content/drive/My Drive/tencent/test1/'+Y_out['DebthInformation']
Y_out['location']='/content/drive/My Drive/tencent/test1/'+Y_out['RGBImage']

filenames_depth=Y_out['file'].to_list()
filenames_img=Y_out['location'].to_list()

#==================define scale
scale_percent = 30 # percent of original size
width = int(1000 * scale_percent / 100)
height = int(830* scale_percent / 100)
dim = (width, height)


#===================image array
X=[]
for i in filenames_img:
    img =cv2.imread(i, 1)
    img_crop=img[150:980,600:1600]   #crop unecessary 
    img=cv2.resize(img_crop,dim, interpolation = cv2.INTER_AREA)
    img=np.array(img, dtype="float64")
    X.append(img)
    
mean_image = np.mean(X, axis=0)
X-= mean_image
X=np.asarray(X)
#np.save('/content/drive/My Drive/tencent/final_test.npy',X)

#=====================Depth array
Depth=[]
for i in filenames_depth:
    depth = cv2.imread(i, -1)
    depth1=depth.astype(float)
    depth_crop=depth1[150:980,600:1600]
    depth_small=cv2.resize(depth_crop,dim,interpolation = cv2.INTER_AREA)
    depth_small=np.array(depth_small, dtype="float64")
    Depth.append(depth_small)

Depth=np.asarray(Depth)
Depth1=Depth[:,:,:,np.newaxis]
print(X.shape)
print(Depth1.shape)

#====================predict================================================
Y_test=model.predict([X,Depth1], batch_size=None, 
        verbose=0, 
        steps=None, 
        callbacks=None,
        max_queue_size=10, 
        workers=1, 
        use_multiprocessing=False)
print(Y_test)
Y=pd.DataFrame(Y_test,columns=['FreshWeightShoot','DryWeightShoot',
                                'Height','Diameter','LeafArea'])

Y.to_excel('/content/drive/My Drive/tencent/result_final.xlsx')

#========================result to JSON============================================
Image_list= list(data["Measurements"].keys())
    
for i in range(len(Y)):
    image=Image_list[i]
    data["Measurements"][image]['FreshWeightShoot']=Y['FreshWeightShoot'][i]
    data["Measurements"][image]['DryWeightShoot']=Y['DryWeightShoot'][i]
    data["Measurements"][image]['Height']=Y['Height'][i]
    data["Measurements"][image]['Diameter']=Y['Diameter'][i]
    data["Measurements"][image]['LeafArea']=Y['LeafArea'][i]

#=====get rid of General
data.pop('General')

file_output='/content/drive/My Drive/tencent/result_new.json'

with open(file_output, 'w') as json_file:
    json.dump(str(data),json_file,sort_keys=True,indent=4,separators=(',', ': '))

#==================================READ IN MODEL 2
Y_2=Y.drop('LeafArea',axis=1)
model2=load_model('/content/drive/My Drive/tencent/Branch_model2.h5',compile=False)
model2.compile(optimizer='adam', loss=custom_loss)   
X_test2=Y_2.to_numpy() 
Y_test2=model2.predict(X_test2,
        verbose=0, 
        steps=None, 
        callbacks=None,
        max_queue_size=10, 
        workers=1, 
        use_multiprocessing=False)

Y_test_result=pd.DataFrame(Y_test2,columns=['LeafArea'])
Y_all=pd.merge(Y_2,Y_test_result,left_index=True,right_index=True)

print(Y_all)
Y_all.to_excel('/content/drive/My Drive/tencent/result_final.xlsx')
Y_all['FreshWeightShoot']=Y_all['FreshWeightShoot'].astype('float64')